{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Kaggle's Titanic Competition\n","\n","https://www.kaggle.com/competitions/titanic\n","\n","Note to self: I actually did most of this as homework for the Machine Learning class from FSU's Engineering in 2021"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import os\n","import regex\n","import string\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from IPython import get_ipython\n","from pathlib import Path\n","from pprint import pprint\n","from sklearn import metrics\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","\n","# Notebook-wide variables\n","datasetsPath = \"data\"\n","figure_width = 18\n","figure_height = 12"]},{"cell_type":"markdown","metadata":{},"source":["## Training Data"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Load training data\n","trainfname = \"train.csv\"\n","trainpath = os.path.join(datasetsPath, trainfname)\n","traindata0 = pd.read_csv(trainpath, index_col=0)"]},{"cell_type":"markdown","metadata":{},"source":["### Convert all string categorical variables to integer categorical values, i.e. One-Hot Encoding\n","\n","Before we can use One-Hot encoding, we must handle missing values, because Sci-kitLearn's One-Hot Encoder can't handle `NaN`s. Only the `Age`, `Cabin`, and `Embarked` variables have `NaN`s in the training data. Note their frequencies"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Survived      0\n","Pclass        0\n","Name          0\n","Sex           0\n","Age         177\n","SibSp         0\n","Parch         0\n","Ticket        0\n","Fare          0\n","Cabin       687\n","Embarked      2\n","dtype: int64\n"]}],"source":["print(traindata0.isna().sum())"]},{"cell_type":"markdown","metadata":{},"source":["#### Missing Value: `Cabin`\n","Because Cabin is specific to each passenger, it does not have much predictive power for new cases, unless we were to extract the level from each value. E.g., C level for cabins C23, C25, and C27.\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2      {C}\n","4      {C}\n","7      {E}\n","11     {G}\n","12     {C}\n","      ... \n","872    {D}\n","873    {B}\n","880    {C}\n","888    {B}\n","890    {C}\n","Length: 204, dtype: object\n"]}],"source":["def extractCabinLevels(dataframe):\n","    cabins = dataframe[\"Cabin\"].dropna()\n","    pattern = r\"(\\w)\\d*\"\n","    index = []\n","    levels = []\n","    for it, string in cabins.iteritems():\n","        cabins = regex.findall(pattern, string)\n","        index.append(it)\n","        # levels.append(', '.join(cabins))\n","        levels.append(set(cabins))\n","    levels = pd.Series(levels, index=index)\n","    return levels\n","\n","\n","alphabet2numbers = {alpha: numeric for (numeric, alpha) in enumerate(string.ascii_uppercase[:], start=1)}\n","\n","\n","def convertCabinLevels(cabinAsSet):\n","    \"\"\"\n","    Uses `alphabet2numbers` defined outside function\n","    \"\"\"\n","    nums = [alphabet2numbers[char] for char in cabinAsSet]\n","    return np.mean(nums)\n","\n","\n","traindata1 = traindata0.copy()\n","levels_train = extractCabinLevels(traindata1)\n","print(levels_train)"]},{"cell_type":"markdown","metadata":{},"source":["We further investigate to see if there are instances of multi-level tickets."]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['A', 'B', 'C', 'D', 'E', 'E, F', 'F', 'G', 'G, F', 'T']\n"]}],"source":["pprint(sorted(levels_train.apply(lambda x: ', '.join(x)).unique()))"]},{"cell_type":"markdown","metadata":{},"source":["After extracting the cabin levels we see that only three cases have multiple levels, these are passengers 76, 700, and 716. For this reason, and because the train and test set have different cabin levels, we will convert this variable into a quantitative one, assuming that the levels represent and ordinal variable."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["levels_quant_train = levels_train.apply(convertCabinLevels)"]},{"cell_type":"markdown","metadata":{},"source":["We replace the values thus:"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["traindata1.loc[levels_quant_train.index, \"Cabin\"] = levels_quant_train.values"]},{"cell_type":"markdown","metadata":{},"source":["As for the missing values, note that the correlation of converted values is with `Fare` is negative. So A (and thus lower numeric values), are worth more. So we assume those not assigned Cabins are because they had lower value tickets. With this logic in mind we assign missing values a numeric value one integer above the maximum ordinal value."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["max_ord_val = len(alphabet2numbers)+1  # 27\n","traindata1[\"Cabin\"] = traindata1[\"Cabin\"].fillna(max_ord_val)"]},{"cell_type":"markdown","metadata":{},"source":["#### Missing Value: `Embarked`\n","Since there are only three values for `Embarked`, we can simply replace the missing values with a dummy value, the string `None`, to create a fourth category."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["traindata1[\"Embarked\"] = traindata1[\"Embarked\"].fillna(\"Embarked_None\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Missing value: `Age`\n","Since the `Age` variable is quantitative, creating a dummy value would not be as useful in this case. Instead, we must find another substitute value. We would prefer to bootstrap the missing values, but will instead use the median age.\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["medianAge_train = traindata1[\"Age\"].median()\n","traindata1[\"Age\"] = traindata1[\"Age\"].fillna(medianAge_train)"]},{"cell_type":"markdown","metadata":{},"source":["#### Missing value: `Fare`\n","The training set has no missing values for this variable, but the test set does. This will be handled later.\n","\n","#### One-Hot Encoding\n","Next we encode the nominal values using the one-hot encoder from `sklearn`.\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["ohe = OneHotEncoder(sparse=False)\n","varsToEncode = [\"Sex\", \"Embarked\"]\n","encs_train = ohe.fit_transform(traindata1[varsToEncode])\n","columns1_train = [f\"{var}_{category}\" for var, array in zip(varsToEncode, ohe.categories_) for category in array]\n","encdf_train = pd.DataFrame(encs_train, index=traindata1.index, columns=columns1_train)"]},{"cell_type":"markdown","metadata":{},"source":["Exclude the encoded columns"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["columns2_train = traindata1.columns.difference(columns1_train)\n","traindata = pd.concat([traindata1[columns2_train], encdf_train], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### Select Regressors\n","We select the variables to use in our analysis. First, we note that the ticket values are alphanumeric. However, this in itself is not a reason to discard teh variable. The concern is that the ticket values are almost exactly unique to each passenger, which provides little inferential value. We also drop `Name` for this reason."]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["xcolumns = traindata.columns.difference({\"Survived\",\n","                                         \"Name\",\n","                                         \"Ticket\"}.union(varsToEncode))\n","\n","X_train0 = traindata[xcolumns]\n","y_train = traindata[\"Survived\"]"]},{"cell_type":"markdown","metadata":{},"source":["### Standardize"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["scaler = StandardScaler().fit(X_train0)\n","X_train = pd.DataFrame(scaler.transform(X_train0), index=X_train0.index, columns=X_train0.columns)"]},{"cell_type":"markdown","metadata":{},"source":["## Test Data"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["testfname = \"test.csv\"\n","testpath = os.path.join(datasetsPath, testfname)\n","testdata0 = pd.read_csv(testpath, index_col=0)"]},{"cell_type":"markdown","metadata":{},"source":["### One-Hot Encoding\n","We process the test data the same as we the training data: replace missing values, convert ordinal values, and encode categorical values"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["testdata1 = testdata0.copy()\n","levels_test = extractCabinLevels(testdata1)\n","levels_quant_test = levels_test.apply(convertCabinLevels)"]},{"cell_type":"markdown","metadata":{},"source":["Compare unique Cabin level values from train and test sets"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['A', 'B', 'C', 'D', 'E', 'E,F', 'F', 'G', 'G,F']\n","['A', 'B', 'C', 'D', 'E', 'E,F', 'F', 'G', 'G,F', 'T']\n"]}],"source":["pprint(sorted(pd.Series([\",\".join(s) for s in levels_test]).unique()))\n","pprint(sorted(pd.Series([\",\".join(s) for s in levels_train]).unique()))"]},{"cell_type":"markdown","metadata":{},"source":["We replace the values thus:"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["testdata1.loc[levels_quant_test.index, \"Cabin\"] = levels_quant_test.values\n","testdata1[\"Cabin\"] = testdata1[\"Cabin\"].fillna(len(alphabet2numbers)+1)\n","testdata1[\"Embarked\"] = testdata1[\"Embarked\"].fillna(\"Embarked_None\")\n","\n","medianAge_test = testdata1[\"Age\"].median()\n","testdata1[\"Age\"] = testdata1[\"Age\"].fillna(medianAge_test)\n","\n","medianFare_test = testdata1[\"Fare\"].median()\n","testdata1[\"Fare\"] = testdata1[\"Fare\"].fillna(medianFare_test)"]},{"cell_type":"markdown","metadata":{},"source":["Encode nominal values"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["encs_test = ohe.transform(testdata1[varsToEncode])\n","columns1_test = [f\"{var}_{category}\" for var, array in zip(varsToEncode, ohe.categories_) for category in array]\n","encdf_test = pd.DataFrame(encs_test, index=testdata1.index, columns=columns1_test)\n","\n","columns2_test = testdata1.columns.difference(varsToEncode)\n","testdata = pd.concat([testdata1[columns2_test], encdf_test], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["### Select regressors"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["X_test0 = testdata[xcolumns]"]},{"cell_type":"markdown","metadata":{},"source":["### Standardize"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["X_test = pd.DataFrame(scaler.transform(X_test0), index=X_test0.index, columns=X_test0.columns)"]},{"cell_type":"markdown","metadata":{},"source":["## Model-building\n","We will compare the results of a logistic regression model and a random forest model.\n","### Logistic Regression\n","#### Train"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["model1 = LogisticRegression().fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["#### Predict"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["accuracy1 = model1.score(X_train, y_train)\n","\n","y_hat_train1 = model1.predict(X_train)\n","accuracy2 = np.mean(y_hat_train1 == y_train)\n","\n","print(accuracy1 == accuracy2)  # True"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We used sklearn's package to perform logistic regression on the variables Age, Cabin, Embarked_C, Embarked_Embarked_None, Embarked_Q, Embarked_S, Fare, Parch, Pclass, Sex_female, Sex_male, and SibSp to achieve a TRAINING accuracy of 0.7991\n"]}],"source":["xcolsa = \", \".join(xcolumns[:-1])\n","xcolsb = xcolumns[-1]\n","print(f\"We used sklearn's package to perform logistic regression on the variables {xcolsa}, and {xcolsb} to achieve a TRAINING accuracy of {accuracy1:0.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest\n","#### Train & Predict"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["model2 = RandomForestClassifier()\n","model2.fit(X_train, y_train)\n","y_hat_train2 = model2.predict(X_train)\n","y_hat_test2 = model2.predict(X_test)\n","\n","train_acc = np.sum(y_train == y_hat_train2)"]},{"cell_type":"markdown","metadata":{},"source":["### Comparison of Results\n","The confusion matrices and ROC curves provide a visual measures of each model's performance.\n","#### Confusion matrices\n","Recall the confusion matrix layout is:\n","|||\n","|-|-|\n","|TP|FN|\n","|FP|TN|"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["               Pred. Positive  Pred. Negative\n","True Positive             469              80\n","True Negative              99             243\n","               Pred. Positive  Pred. Negative\n","True Positive             547               2\n","True Negative              13             329\n"]}],"source":["conf_mat1 = pd.DataFrame(metrics.confusion_matrix(y_train, y_hat_train1), columns=[\"Pred. Positive\", \"Pred. Negative\"], index=[\"True Positive\", \"True Negative\"])\n","print(conf_mat1)\n","conf_mat2 = pd.DataFrame(metrics.confusion_matrix(y_train, y_hat_train2), columns=[\"Pred. Positive\", \"Pred. Negative\"], index=[\"True Positive\", \"True Negative\"])\n","print(conf_mat2)"]},{"cell_type":"markdown","metadata":{},"source":["#### ROC Curves\n","The ROC curve is made from the training set, because we need the true $y$ values, which are not available for the test set."]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"image/png":"","text/plain":["<Figure size 1296x864 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"},{"data":{"image/png":"","text/plain":["<Figure size 1296x864 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Logistic Regression\n","y_score1 = model1.decision_function(X_train)\n","fpr1, tpr1, thresholds1 = metrics.roc_curve(y_train, y_score1)\n","roc_auc = metrics.auc(fpr1, tpr1)\n","fig1 = plt.figure()\n","fignum = fig1.number\n","lw = 2\n","plt.plot(fpr1, tpr1, color='darkorange',\n","         lw=lw, label=f'ROC curve (area = {roc_auc:0.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic on training data')\n","plt.legend(loc=\"lower right\")\n","fig1.set_size_inches([figure_width, figure_height])\n","plt.show()\n","\n","# Random Forest\n","y_score2 = model2.predict_proba(X_train)[:,1]\n","fpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, y_score2)\n","roc_auc2 = metrics.auc(fpr2, tpr2)\n","fig2 = plt.figure()\n","fignum2 = fig2.number\n","lw = 2\n","plt.plot(fpr2, tpr2, color='darkorange',\n","         lw=lw, label=f'ROC curve (area = {roc_auc2:0.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver operating characteristic on training data (Random Forest)')\n","plt.legend(loc=\"lower right\")\n","fig2.set_size_inches([figure_width, figure_height])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Competition Submission\n","We choose the model we want to use to submit our test set preidction submission"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# yhat_test = model1.predict(X_test)\n","# print(yhat_test)"]}],"metadata":{"interpreter":{"hash":"c0150ad9800bc6dcdab039921ffbe6eb8efd7b9bba92461693bcaecb5cae1aaa"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":2}
